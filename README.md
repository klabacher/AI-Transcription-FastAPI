# ğŸš€ API de TranscriÃ§Ã£o Otimizada

[![Feito com FastAPI](https://img.shields.io/badge/Feito%20com-FastAPI-blue.svg)](https://fastapi.tiangolo.com/)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/release/python-3100/)
[![LicenÃ§a: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

Uma API de transcriÃ§Ã£o de Ã¡udio de alta performance, construÃ­da com FastAPI e focada em oferecer acesso a modelos de *state-of-the-art* como `faster-whisper` e versÃµes otimizadas do Hugging Face.

O projeto foi desenhado para ser robusto e eficiente, utilizando workers em processos isolados para lidar com o processamento pesado de IA, garantindo que a API principal permaneÃ§a sempre responsiva.

*(Um GIF demonstrando a interface web seria Ã³timo aqui!)*

## âœ¨ Features Principais

- **Workers Persistentes**: Modelos de IA sÃ£o carregados uma Ãºnica vez em processos worker separados, eliminando o tempo de carregamento a cada requisiÃ§Ã£o e otimizando o uso de recursos (CPU/GPU).
- **Sistema de Fila de Jobs**: As tarefas de transcriÃ§Ã£o sÃ£o enfileiradas e processadas de forma assÃ­ncrona, permitindo que a API lide com um grande volume de requisiÃ§Ãµes.
- **DetecÃ§Ã£o de Hardware**: A API detecta automaticamente a presenÃ§a de uma GPU (CUDA) e suas capacidades (como suporte a FP16) para ativar os modelos mais performÃ¡ticos.
- **Setup Automatizado**: Na primeira execuÃ§Ã£o, um script de setup baixa e armazena em cache todos os modelos de IA necessÃ¡rios, agilizando as inicializaÃ§Ãµes futuras.
- **Interface de Testes (UI)**: Uma interface web simples e funcional (`/ui`) para testar a API, enviar arquivos, acompanhar o progresso dos jobs e visualizar os resultados.
- **Processamento de MÃºltiplos Arquivos e .ZIP**: Envie vÃ¡rios arquivos de Ã¡udio ou um Ãºnico arquivo `.zip` contendo os Ã¡udios para criar mÃºltiplos jobs de uma sÃ³ vez.
- **Monitoramento e Gerenciamento**: Endpoints para verificar o status de jobs especÃ­ficos, cancelar tarefas em andamento e limpar jobs antigos automaticamente.

## ğŸ› ï¸ Stack e Escolhas de Arquitetura

A escolha das tecnologias e da arquitetura foi pensada para criar um sistema desacoplado e escalÃ¡vel.

- **FastAPI**: Escolhido pela sua alta performance, documentaÃ§Ã£o automÃ¡tica (Swagger UI) e sintaxe moderna com `async/await`.
- **Multiprocessing**: A decisÃ£o mais crÃ­tica da arquitetura. Usamos o mÃ³dulo `multiprocessing` do Python para isolar os workers de IA do processo principal da API. Isso evita que o consumo intenso de memÃ³ria e CPU dos modelos de transcriÃ§Ã£o trave o servidor web, garantindo que a API esteja sempre disponÃ­vel para receber novas requisiÃ§Ãµes.
- **Gerenciamento de Ciclo de Vida (Lifespan)**: O FastAPI `lifespan` Ã© usado para iniciar os pools de workers e a thread de limpeza (`janitor`) quando a API sobe, e para garantir um desligamento gracioso, finalizando todos os processos de forma segura.
- **Vanilla JS/HTML/CSS**: A interface foi mantida intencionalmente simples, sem frameworks complexos, para ser leve e fÃ¡cil de entender, focando na funcionalidade.
- **IA e Ãudio**:
    - **`faster-whisper`**: Para transcriÃ§Ãµes otimizadas de alta velocidade em CPU e GPU.
    - **`transformers`**: Para carregar modelos do Hugging Face Hub, como o `distil-whisper`.
    - **`torch`**: A base para todos os modelos de IA.
    - **`soundfile`**: Para ler informaÃ§Ãµes de metadados dos arquivos de Ã¡udio, como a duraÃ§Ã£o.

## ğŸ§  Modelos DisponÃ­veis

A API disponibiliza os seguintes modelos, ativados conforme o hardware detectado:

| Model ID | ImplementaÃ§Ã£o | Requer GPU? | DescriÃ§Ã£o |
| :--- | :--- | :--- | :--- |
| `distil_large_v3_ptbr` | Hugging Face | â– NÃ£o | Recomendado para testes locais. Ã“tima qualidade em PT-BR, leve e rÃ¡pido em CPU. |
| `faster_medium_fp16` | faster-whisper | âœ… Sim | Excelente equilÃ­brio entre velocidade e qualidade em GPU. |
| `faster_large-v3_fp16` | faster-whisper | âœ… Sim | MÃ¡xima qualidade e precisÃ£o em PT-BR. Requer GPU potente (VRAM > 8GB). |
| `faster_large-v3_int8` | faster-whisper | â– NÃ£o | Qualidade do Large-v3 com menor uso de memÃ³ria. Ideal para CPUs potentes ou GPUs com VRAM limitada. |

## ğŸš€ Como Usar

### PrÃ©-requisitos
- Python 3.10 ou superior.
- (Opcional, mas recomendado) Uma placa de vÃ­deo NVIDIA com drivers CUDA instalados para performance mÃ¡xima.

### InstalaÃ§Ã£o e ExecuÃ§Ã£o
1.  **Clone o repositÃ³rio:**
    ```bash
    git clone [https://github.com/seu-usuario/seu-repositorio.git](https://github.com/seu-usuario/seu-repositorio.git)
    cd seu-repositorio
    ```

2.  **Crie e ative um ambiente virtual:**
    ```bash
    python -m venv .venv
    # Windows
    .venv\Scripts\activate
    # macOS / Linux
    source .venv/bin/activate
    ```

3.  **Instale as dependÃªncias:**
    ```bash
    pip install -r requirements.txt
    ```

4.  **Inicie a API:**
    ```bash
    uvicorn main:app --reload
    ```
    > âš ï¸ **AtenÃ§Ã£o na Primeira ExecuÃ§Ã£o!**
    > Na primeira vez que vocÃª iniciar a API, um script de setup serÃ¡ executado para baixar todos os modelos de IA. Este processo pode demorar **vÃ¡rios minutos** e consumir um espaÃ§o considerÃ¡vel em disco. As inicializaÃ§Ãµes seguintes serÃ£o quase instantÃ¢neas.

5.  **Acesse a aplicaÃ§Ã£o:**
    - **Interface Web**: Abra seu navegador em `http://127.0.0.1:8000/ui`
    - **DocumentaÃ§Ã£o da API**: Acesse `http://127.0.0.1:8000/docs`

## ğŸ“¡ Endpoints da API

- `GET /`: Retorna uma mensagem de status e informaÃ§Ãµes sobre o hardware detectado.
- `GET /models`: Lista os modelos de transcriÃ§Ã£o que estÃ£o ativos e compatÃ­veis com o hardware atual.
- `GET /queues`: Monitora o status de todos os jobs na fila.
- `POST /jobs`: Cria um ou mais jobs de transcriÃ§Ã£o. Recebe `model_id`, `language`, `session_id` e os arquivos de Ã¡udio (`files`).
- `GET /jobs/{job_id}`: Verifica o status, progresso e resultado de um job especÃ­fico.
- `POST /jobs/{job_id}/cancel`: Solicita o cancelamento de um job que estÃ¡ na fila ou em processamento.
- `GET /jobs/{job_id}/download`: Baixa o resultado da transcriÃ§Ã£o em um arquivo `.txt`.

## â¤ï¸ CrÃ©ditos e Agradecimentos

Este projeto sÃ³ Ã© possÃ­vel graÃ§as ao incrÃ­vel trabalho da comunidade open-source.

- **Modelos de IA**:
    - **Whisper e faster-whisper**: Agradecimentos Ã  [OpenAI](https://openai.com/) pelo modelo Whisper original e a [Guillaume Klein (SYSTRAN)](https://github.com/guillaumekln/faster-whisper) pela implementaÃ§Ã£o otimizada `faster-whisper`.
    - **distil-whisper-large-v3-ptbr**: Obrigado ao usuÃ¡rio [freds0](https://huggingface.co/freds0) do Hugging Face por treinar e disponibilizar a versÃ£o destilada para portuguÃªs.

- **Ferramentas**:
    - [FastAPI](https://fastapi.tiangolo.com/) por ser um framework web fantÃ¡stico.
    - [PyTorch](https://pytorch.org/) e [Hugging Face](https://huggingface.co/) por democratizarem o acesso a modelos de Machine Learning.

---

> **Aviso**: Este projeto foi desenvolvido com o auxÃ­lio de InteligÃªncia Artificial para acelerar a criaÃ§Ã£o de cÃ³digo, resolver problemas e gerar documentaÃ§Ã£o.