{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Transcription FastAPI - V3 Demo\n",
    "\n",
    "This notebook demonstrates how to run and interact with the V3 version of the AI Transcription API directly within a Google Colab environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the V3 Architecture\n",
    "\n",
    "The V3 architecture is designed for scalability and separates the application into two main services:\n",
    "\n",
    "1.  **API Service (`api`)**: A lightweight FastAPI server that accepts requests, manages jobs, and handles caching.\n",
    "2.  **Worker Service (`worker`)**: A dedicated process that consumes jobs from a queue, performs the heavy lifting of AI transcription, and reports results back.\n",
    "\n",
    "This separation allows each component to be scaled independently in a production environment. To manage communication, V3 uses **Redis** as a central message broker, state database, and cache."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution Modes: `distributed` vs. `local`\n",
    "\n",
    "The V3 application supports two execution backends, configured via the `EXECUTION_BACKEND` environment variable:\n",
    "\n",
    "-   **`distributed` (Default for Production)**: In this mode, the API and Worker services are completely separate. The API publishes jobs to a Redis Stream, and one or more independent workers consume jobs from that stream. This is the recommended mode for `Docker` and production deployments.\n",
    "\n",
    "-   **`local` (For this Demo)**: In this mode, the API service directly starts a transcription job in a new local process using Python's `multiprocessing`. This avoids the need for a separate Redis server and is suitable for local development or single-machine environments like this Colab notebook.\n",
    "\n",
    "**For this demonstration, we will use the `local` backend** as it allows us to run the entire application within the confines of this single notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# --- Environment Variables ---\n",
    "# We must set these before importing the application code\n",
    "\n",
    "# Use the 'local' backend for this demo\n",
    "os.environ['EXECUTION_BACKEND'] = 'local'\n",
    "\n",
    "# Set a dummy API Key\n",
    "os.environ['API_KEY'] = 'colab-secret-key'\n",
    "\n",
    "# Set a log level\n",
    "os.environ['LOG_LEVEL'] = 'INFO'\n",
    "\n",
    "print('Environment variables set for LOCAL mode.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all required packages\n",
    "# Note: This will install PyTorch, which can take a few minutes.\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Run the API Server\n",
    "\n",
    "Now we will run the FastAPI application in the background. We'll use `uvicorn` to serve the app and `threading` to run it without blocking the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uvicorn\n",
    "import threading\n",
    "from main import app\n",
    "\n",
    "class AppServer(threading.Thread):\n",
    "    def __init__(self):\n",
    "        threading.Thread.__init__(self)\n",
    "    def run(self):\n",
    "        uvicorn.run(app, host='0.0.0.0', port=8000, log_level='info')\n",
    "\n",
    "server_thread = AppServer()\n",
    "server_thread.daemon = True\n",
    "server_thread.start()\n",
    "\n",
    "print('API server is running in the background on port 8000.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Send a Transcription Job\n",
    "\n",
    "With the server running, we can now act as a client and send a request to the `/jobs` endpoint. We'll create a dummy audio file for this purpose.\n",
    "\n",
    "Since we are in `local` mode, the API will receive the request and start a new process to handle the transcription. You can see the logs from both the API and the `[LocalWorker]` in the notebook's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "# Create a dummy WAV file for testing (silence)\n",
    "import scipy.io.wavfile\n",
    "import numpy as np\n",
    "\n",
    "samplerate = 16000\n",
    "duration = 2 # seconds\n",
    "frequency = 440.0 # A4\n",
    "t = np.linspace(0., duration, int(samplerate * duration))\n",
    "amplitude = np.iinfo(np.int16).max * 0.5\n",
    "data = amplitude * np.sin(2. * np.pi * frequency * t)\n",
    "dummy_audio_path = 'test_audio.wav'\n",
    "scipy.io.wavfile.write(dummy_audio_path, samplerate, data.astype(np.int16))\n",
    "\n",
    "print(f'Created dummy audio file: {dummy_audio_path}')\n",
    "\n",
    "# --- Send Request ---\n",
    "url = 'http://localhost:8000/jobs'\n",
    "\n",
    "api_key = os.environ['API_KEY']\n",
    "headers = {'X-API-Key': api_key}\n",
    "\n",
    "payload = {\n",
    "    'model_id': 'distil_large_v3_ptbr',\n",
    "    'session_id': 'colab-session-123',\n",
    "    'language': 'pt'\n",
    "}\n",
    "\n",
    "files = {'files': open(dummy_audio_path, 'rb')}\n",
    "\n",
    "try:\n",
    "    response = requests.post(url, headers=headers, data=payload, files=files)\n",
    "    response.raise_for_status() # Raise an exception for bad status codes\n",
    "    \n",
    "    print(f'API Response Status: {response.status_code}')\n",
    "    print('API Response Body:')\n",
    "    print(response.json())\n",
    "    \n",
    "    print('\\n---')\n",
    "    print('Job was dispatched! Check the logs above to see the [LocalWorker] process the job.')\n",
    "    print('Since local mode is fire-and-forget, we cannot check job status via the API.')\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f'An error occurred: {e}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
